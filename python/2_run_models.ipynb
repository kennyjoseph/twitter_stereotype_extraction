{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from user import *\n",
    "import random\n",
    "import msgpack\n",
    "import glob \n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import msgpack\n",
    "from twitter_dm.utility.general_utils import mkdir_no_err\n",
    "from twitter_dm.utility.general_utils import tab_stringify_newline as tsn\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "float_formatter = lambda x: \"%.6f\" % x\n",
    "np.set_printoptions(threshold=10000,\n",
    "                    linewidth=100,\n",
    "                    formatter={'float_kind':float_formatter})\n",
    "\n",
    "# create the output directory for these runs\n",
    "OUTPUT_DIRECTORY = \"new_recent\"\n",
    "mkdir_no_err(OUTPUT_DIRECTORY)\n",
    "\n",
    "msgpack_sent_data_dir = os.path.join(OUTPUT_DIRECTORY,\"sent_dat\")\n",
    "mkdir_no_err(msgpack_sent_data_dir)\n",
    "\n",
    "#some constants\n",
    "MIN_TWEETS_PER_USER = 50\n",
    "PERCENT_TWEETS_TO_USE_FOR_TEST_DATA = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constraints import IDENTITY_PREFIX\n",
    "identities = [x.strip() for x in io.open(\"../data/identity_data/final_identities_list.txt\").readlines()]\n",
    "identity_to_id = {identity : IDENTITY_PREFIX+str(i) for i, identity in enumerate(identities)}\n",
    "id_to_identity = {v : k for k, v in identity_to_id.items()}\n",
    "\n",
    "sent_dict = {}\n",
    "for x in io.open(\"../data/sentiment_data/clean_epa_terms.txt\"):\n",
    "    x_spl = x.split(\"\\t\")\n",
    "    sent_dict[x_spl[0]] = [float(z) for z in x_spl[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the first time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load in the users\n",
    "files = glob.glob(os.path.join(OUTPUT_DIRECTORY,\"textunits/*.mpack\"))\n",
    "print len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for i,fil in enumerate(files):\n",
    "    if i % 100 == 0:\n",
    "        print i\n",
    "    try:\n",
    "        users += get_users_from_mpack_fil(fil, MIN_TWEETS_PER_USER, PERCENT_TWEETS_TO_USE_FOR_TEST_DATA)\n",
    "    except:\n",
    "        print i, fil\n",
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct identity dictionary\n",
    "texts = [u.all_identities for u in users]\n",
    "dictionary = corpora.Dictionary(texts) \n",
    "print dictionary\n",
    "training_corpus = [dictionary.doc2bow(u.training_id) for u in users]\n",
    "training_corpus_matrix = corpus2dense(training_corpus,len(dictionary))\n",
    "test_corpus = [dictionary.doc2bow(u.test_id) for u in users]\n",
    "test_corpus_matrix = corpus2dense(test_corpus,len(dictionary))\n",
    "for i,u in enumerate(users):\n",
    "    u.training_vector = training_corpus_matrix[:,i]\n",
    "    u.test_vector = test_corpus_matrix[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_users = users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The association model estimation appears to be unstable for users w/ < 175 or so observations,\n",
    "# so remove these users from the data\n",
    "users =[u for u in users if u.training_vector.sum() > 175]\n",
    "len(full_users), len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can now cache the users for future analysis/ if something weird happens\n",
    "dump_users(users, OUTPUT_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together some more indexes\n",
    "index_to_id = {k : v for k,v in dictionary.items()}\n",
    "id_to_index = {v : k for k,v in dictionary.items()}\n",
    "index_to_identity = {k : id_to_identity[v] for k, v in dictionary.items()}\n",
    "\n",
    "# write out the index for analysis\n",
    "out_ind_id = io.open(os.path.join(OUTPUT_DIRECTORY,\"index_to_identity_final.tsv\"),\"w\")\n",
    "for k, v in index_to_identity.items():\n",
    "    out_ind_id.write(unicode(k) + \"\\t\" + v + \"\\n\")\n",
    "out_ind_id.close()\n",
    "\n",
    "out_ind_id = io.open(os.path.join(OUTPUT_DIRECTORY,\"index_to_id_final.tsv\"),\"w\")\n",
    "for k, v in index_to_id.items():\n",
    "    out_ind_id.write(unicode(k) + \"\\t\" + v + \"\\n\")\n",
    "out_ind_id.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading user data...\n",
      " ... \n",
      " loaded! \n"
     ]
    }
   ],
   "source": [
    "users = list(load_users(OUTPUT_DIRECTORY))\n",
    "\n",
    "index_to_identity = {int(line.split(\"\\t\")[0]) : line.strip().split(\"\\t\")[1] \n",
    "               for line in open(os.path.join(OUTPUT_DIRECTORY,\"index_to_identity_final.tsv\")) }\n",
    "\n",
    "index_to_id = {k:identity_to_id[v] for k, v in index_to_identity.items()}\n",
    "id_to_index = {v : k for k,v in index_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set identity sentiment values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'african american',\n",
       " u'bf',\n",
       " u'hispanic',\n",
       " u'jew',\n",
       " u'latino',\n",
       " u'lawmaker',\n",
       " u'protester',\n",
       " u'protestor'}"
      ]
     },
     "execution_count": 7,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "identities_with_no_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sent_dict['nerd'] = [0.337138927097661,1.54855305466238,-0.326494345718901]\n",
    "sent_dict['geek'] = [1.13780012584526,0.771012312261079,-1.15146349297431]\n",
    "\n",
    "# for those we have in our dictionarys, we can just use that\n",
    "identity_values = {k : sent_dict[v] for k,v in id_to_identity.items() if v in sent_dict}\n",
    "# for those we dont, we use empirical priors, generated from a 1% sample - see gen_empirical_priors_unk.py\n",
    "empirical_prior_data = pd.read_csv(\"../data/sentiment_data/data_for_empirical_priors.tsv\", header=None,sep=\"\\t\")\n",
    "empirical_prior_data.columns= ['user','word','epa','total_sent','n_tweets']\n",
    "\n",
    "# make sure we're getting everything\n",
    "identities_with_no_values = set([x for x in id_to_identity.values() if x not in sent_dict])\n",
    "identities_in_empirical = set(empirical_prior_data.word.unique().tolist())\n",
    "print identities_with_no_values - identities_in_empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not a lot of tweets for the prior but good enough\n",
    "k = empirical_prior_data.groupby(['word']).apply(lambda x: x['n_tweets'].sum()/3)\n",
    "# print k \n",
    "\n",
    "# do empirical bayes ish for these priors ...\n",
    "dat = empirical_prior_data.groupby(['word','epa']).apply(lambda x: np.mean(x['total_sent']/x['n_tweets'])).reset_index()\n",
    "dat.columns = ['word','epa','value']\n",
    "# do some linear transformations to correct for known biases between survey and sentiment model\n",
    "\n",
    "dat['e_trans'] = (dat[dat.epa =='e'].value - .18) * 4.3 / np.max(np.abs(dat[dat.epa=='e'].value))\n",
    "dat['p_trans'] = (dat[dat.epa =='p'].value - .56) * 4.3 / np.max(np.abs(dat[dat.epa=='p'].value))\n",
    "dat['a_trans'] = (dat[dat.epa =='a'].value +.57) * 4.3 / np.max(np.abs(dat[dat.epa=='a'].value))\n",
    "\n",
    "for identity in identities_with_no_values:\n",
    "    id = identity_to_id[identity]\n",
    "    e = dat.ix[(dat.word == identity) & (dat.epa == 'e')].e_trans.tolist()[0]\n",
    "    p = dat.ix[(dat.word == identity) & (dat.epa == 'p')].p_trans.tolist()[0]\n",
    "    a = dat.ix[(dat.word == identity) & (dat.epa == 'a')].a_trans.tolist()[0]\n",
    "    identity_values[id]= [e,p,a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out raw text for Vader baseline sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the raw text so we can build the baseline sentiment model elsewhere\n",
    "training_raw_text_out = io.open(os.path.join(OUTPUT_DIRECTORY, \"training_raw_text_fin.txt\"),\"w\")\n",
    "for j, u in enumerate(users):\n",
    "    if j % 10000 == 0:\n",
    "        print j\n",
    "    uid = u.uid\n",
    "    for i, raw_text in enumerate(u.training_raw_text):\n",
    "        training_raw_text_out.write(tsn([uid,\n",
    "                                         u\",\".join(u.training_identities_per_tweet[i]),\n",
    "                                         raw_text.decode(\"utf8\")]))\n",
    "training_raw_text_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal w/ pyspark pickling issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because pyspark uses pickle, which is really slow for this data, we cache data on disk ourselves using msgpack\n",
    "# in practice this is infinitely more efficient, although theoretically it should be slower\n",
    "spl_inds = np.array_split(np.array(range(len(users))),256)\n",
    "\n",
    "last_spl = 0\n",
    "for i, inds in enumerate(spl_inds):\n",
    "    max_spl_inds = last_spl + len(inds)\n",
    "    for q, name in [[[u.training_id_to_tweets for u in users[last_spl:max_spl_inds]], 'tr_inds'],\n",
    "                    [[u.training_deflection_strings  for u in users[last_spl:max_spl_inds]], \"tr_def_str\" ],\n",
    "                    [[u.test_deflection_strings  for u in users[last_spl:max_spl_inds]], \"tes_def_str\"],\n",
    "                    [[u.training_identities_per_tweet  for u in users[last_spl:max_spl_inds]],'tr_id_per_tw'],\n",
    "                    [[u.test_identities_per_tweet  for u in users[last_spl:max_spl_inds]],'tes_id_per_tw']\n",
    "                   ]:\n",
    "        msgpack.dump(q, open(os.path.join(msgpack_sent_data_dir,name+str(i)+\".mpack\"),\"wb\"))\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print i\n",
    "    last_spl = max_spl_inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up spark\n",
    "conf = (SparkConf().setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "        .set(\"spark.local.dir\", \"/usr1/kjoseph/spark_tmp\")\n",
    "        .set(\"spark.driver.maxResultSize\", \"80g\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentiment_model\n",
    "sentiment_model = reload(sentiment_model)\n",
    "sm = sentiment_model.SentimentModel(len(users), index_to_id, identity_values,kappa=300.,nu=3000.)\n",
    "sent_output_dir = os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\")\n",
    "mkdir_no_err(sent_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO RELOAD PREVIOUS SENTIMENT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = sentiment_model.SentimentModel(len(users), index_to_id, identity_values,kappa=100.,nu=1000)\n",
    "\n",
    "iteration = str(250)\n",
    "\n",
    "basic_info = msgpack.load(open(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",iteration+\"_sent_basic.mpack\"))\n",
    "\n",
    "mu = np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",iteration+\"_sent_mu.npy\"))\n",
    "phi = np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",iteration+\"_sent_phi.npy\"))\n",
    "prec_mat = np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",iteration+\"_sent_precision_matrix.npy\"))\n",
    "sigma = np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",iteration+\"_sent_sigma.npy\"))\n",
    "sigma0 = np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",\"sent_sigma_0.npy\"))\n",
    "mu=  np.load(os.path.join(OUTPUT_DIRECTORY,\"sent_res_final\",\"sent_mu_0.npy\"))\n",
    "\n",
    "sm.kappa = basic_info['kappa']\n",
    "sm.iteration = basic_info['iteration']\n",
    "sm.index_to_ids = basic_info['index_to_ids']\n",
    "sm.beta = basic_info['beta']\n",
    "sm.ids_to_index = basic_info['ids_to_index']\n",
    "sm.n_identity_sent_values = basic_info['n_identity_sent_values ']\n",
    "sm.nu = basic_info['nu']\n",
    "sm.identity_to_values_small = basic_info['identity_to_values_small']\n",
    "sm.mu = mu\n",
    "sm.phi = phi    \n",
    "sm.precision_matrix = prec_mat\n",
    "sm.sigma = sigma\n",
    "sm.sigma_0 = sigma0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the models\n",
    "\n",
    "# after about 300 iterations the model's perplexity stabilizes, so we quit.\n",
    "for i in range(200):\n",
    "    print i, sm.iteration\n",
    "    if i % 50 == 0:\n",
    "        sm.dump(sent_output_dir)\n",
    "    #if i > 500:\n",
    "    np.save(os.path.join(sent_output_dir,str(sm.iteration)+\"_mu\"), sm.mu)\n",
    "    sm.iterate(sc,spl_inds,msgpack_sent_data_dir,id_to_index )\n",
    "\n",
    "sm.dump(sent_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print sm.iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import association_model\n",
    "association_model = reload(association_model)\n",
    "am = association_model.AssociationModel(users,len(id_to_index))\n",
    "assoc_output_dir = os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\")\n",
    "mkdir_no_err(assoc_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To reload previous association model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload assoc model\n",
    "am = association_model.AssociationModel(users,len(id_to_index))\n",
    "\n",
    "iteration = str(883)\n",
    "\n",
    "basic_info = msgpack.load(open(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",iteration+\"_assoc_basic.mpack\")))\n",
    "\n",
    "mu = np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",iteration+\"_assoc_mu.npy\"))\n",
    "eta = np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",iteration+\"_assoc_eta.npy\"))\n",
    "prec_mat = np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",iteration+\"_assoc_precision_matrix.npy\"))\n",
    "sigma = np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",iteration+\"_assoc_sigma.npy\"))\n",
    "W = np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",\"assoc_W.npy\"))\n",
    "mu0=  np.load(os.path.join(OUTPUT_DIRECTORY,\"assoc_res_final\",\"assoc_mu_0.npy\"))\n",
    "\n",
    "am.iteration = basic_info['iteration']\n",
    "am.mu = mu\n",
    "am.eta = eta\n",
    "am.precision_matrix = prec_mat\n",
    "am.sigma = sigma\n",
    "am.W = W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "883\n"
     ]
    }
   ],
   "source": [
    "print am.iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run association model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ASSOC MODEL LL TRAINING, TEST  -10613438.457 , -4761764.64643\n",
      "1"
     ]
    }
   ],
   "source": [
    "# initialize the models \n",
    "## after about 300 iterations the LL starts to increase ... indicates its doing some kind of overfitting\n",
    "\n",
    "for i in range(500):\n",
    "    print i \n",
    "    if am.iteration % 50 == 0:\n",
    "        am.dump(assoc_output_dir)\n",
    "    #if i > 300:\n",
    "    np.save(os.path.join(assoc_output_dir,str(am.iteration)+\"_mu\"), am.mu)\n",
    "    am.iterate(sc)\n",
    "\n",
    "am.dump(assoc_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}