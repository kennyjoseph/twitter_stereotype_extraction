{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import io\n",
    "import sys\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "\n",
    "from user import *\n",
    "import glob\n",
    "import cPickle as pickle\n",
    "import os\n",
    "from twitter_dm.utility.general_utils import read_grouped_by_newline_file\n",
    "from twitter_dm.utility.general_utils import tab_stringify_newline as tsn\n",
    "from collections import defaultdict\n",
    "from textunit import TextUnit\n",
    "\n",
    "from constraints import get_id_and_value_map\n",
    "from constraints import IDENTITY_PREFIX, SENTWORD_PREFIX\n",
    "\n",
    "from math import log\n",
    "from vaderSentiment.vaderSentiment import sentiment\n",
    "from pred_models import *\n",
    "SMOOTHING_PARAM = 1\n",
    "n_identities = len(identities)\n",
    "from stat_util import *\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import codecs\n",
    "from vaderSentiment.vaderSentiment import sentiment\n",
    "from twitter_dm.nlp.Tokenize import extract_tokens_twokenize_and_regex\n",
    "\n",
    "\n",
    "sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "float_formatter = lambda x: \"%.6f\" % x\n",
    "np.set_printoptions(threshold=10000,\n",
    "                    linewidth=100,\n",
    "                    formatter={'float_kind':float_formatter})\n",
    "\n",
    "output_dir = \"new_recent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = [x.strip() for x in io.open(\"../data/identity_data/final_identities_list.txt\").readlines()]\n",
    "identity_to_id = {identity : IDENTITY_PREFIX+str(i) for i, identity in enumerate(identities)}\n",
    "id_to_identity = {v : k for k, v in identity_to_id.items()}\n",
    "\n",
    "index_to_id = {int(x.split(\"\\t\")[0]):x.strip().split(\"\\t\")[1] \n",
    "               for x in  io.open(os.path.join(output_dir,\"index_to_id_final.tsv\"))}\n",
    "id_to_index = {v : k for k,v in index_to_id.items()}\n",
    "\n",
    "ids_in_index_order = [None] * len(identities)\n",
    "for k, v in index_to_id.items():\n",
    "    ids_in_index_order[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading user data...\n",
      " ... \n",
      " loaded! \n"
     ]
    }
   ],
   "source": [
    "users = list(load_users(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44896"
      ]
     },
     "execution_count": 11,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = io.open(os.path.join(output_dir, \"user_uids.txt\"),\"w\")\n",
    "for u in users:\n",
    "    dout.write(unicode(u.uid) + \"\\n\")\n",
    "dout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# construct simple sentiment info for simple sent models\n",
    "i2ind = pd.DataFrame(id_to_index.items())\n",
    "i2ind.columns = ['iden_id','index']\n",
    "simple_sent_data = pd.read_csv(os.path.join(output_dir,\"user_to_identity_to_simple_sent_val.csv\"))\n",
    "\n",
    "overall_simple_sent = simple_sent_data[['iden_id','val']].groupby(\"iden_id\").mean().reset_index()\n",
    "overall_simple_sent = pd.merge(overall_simple_sent,i2ind, on='iden_id')\n",
    "simple_sent_vector = np.zeros(len(i2ind))\n",
    "for x in overall_simple_sent.values.tolist():\n",
    "    simple_sent_vector[x[2]] = x[1]\n",
    "\n",
    "user_simple_sent_data = defaultdict(dict)\n",
    "for x in simple_sent_data.values.tolist():\n",
    "    if x[2] in id_to_index:\n",
    "        user_simple_sent_data[x[1]][x[2]] = x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in some other data we're going to need\n",
    "training_user_counts = np.zeros((len(users),len(identity_to_id)))\n",
    "test_user_counts = np.zeros((len(users), len(identity_to_id)))\n",
    "for i, u in enumerate(users):\n",
    "    training_user_counts[i] = u.training_vector\n",
    "    test_user_counts[i] = u.test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up spark\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (SparkConf().setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "        .set(\"spark.local.dir\", \"/usr1/kjoseph/spark_tmp\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dout = io.open(os.path.join(output_dir, \"sent_ids_list.txt\"),\"w\")\n",
    "for s in sent_index_to_ids:\n",
    "    dout.write(unicode(s) + \"\\n\")\n",
    "dout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\tour_assoc\t-4.36053189407\t0.0\n",
      "\n",
      "500\tsimp_assoc\t-4.86362129137\t0.0\n",
      "\n",
      "500\tuser_assoc\t-4.4740504394\t0.0\n",
      "\n",
      "500\tsent_basic\t-5.71380834378\t134.743815854\n",
      "\n",
      "500\tu_sent\t-5.68739314868\t127.27178484\n",
      "\n",
      "500\tour_sent\t-10.6285375335\t125.952145055\n",
      "\n",
      "500 done\n",
      "600\tour_assoc\t-4.36387704663\t0.0\n",
      "\n",
      "600\tsimp_assoc\t-4.86362129137\t0.0\n",
      "\n",
      "600\tuser_assoc\t-4.4740504394\t0.0\n",
      "\n",
      "600\tsent_basic\t-5.71380834378\t134.743815854\n",
      "\n",
      "600\tu_sent\t-5.68739314868\t127.27178484\n",
      "\n",
      "600\tour_sent\t-10.6302321885\t126.013852603\n",
      "\n",
      "600 done\n",
      "700\tour_assoc\t-4.36355148217\t0.0\n",
      "\n",
      "700\tsimp_assoc\t-4.86362129137\t0.0\n",
      "\n",
      "700\tuser_assoc\t-4.4740504394\t0.0\n",
      "\n",
      "700\tsent_basic\t-5.71380834378\t134.743815854\n",
      "\n",
      "700\tu_sent\t-5.68739314868\t127.27178484\n",
      "\n",
      "700\tour_sent\t-10.6288611643\t126.026549933\n",
      "\n",
      "700 done\n",
      "800\tour_assoc\t-4.36482763681\t0.0\n",
      "\n",
      "800\tsimp_assoc\t-4.86362129137\t0.0\n",
      "\n",
      "800\tuser_assoc\t-4.4740504394\t0.0\n",
      "\n",
      "800\tsent_basic\t-5.71380834378\t134.743815854\n",
      "\n",
      "800\tu_sent\t-5.68739314868\t127.27178484\n",
      "\n",
      "800\tour_sent\t-10.6294040502\t126.09244405\n",
      "\n",
      "800 done\n",
      "900\tour_assoc\t-4.36369729517\t0.0\n",
      "\n",
      "900\tsimp_assoc\t-4.86362129137\t0.0\n",
      "\n",
      "900\tuser_assoc\t-4.4740504394\t0.0\n",
      "\n",
      "900\tsent_basic\t-5.71380834378\t134.743815854\n",
      "\n",
      "900\tu_sent\t-5.68739314868\t127.27178484\n",
      "\n",
      "900\tour_sent\t-10.6298173438\t126.125718209\n",
      "\n",
      "900 done\n"
     ]
    }
   ],
   "source": [
    "of = open(\"results.tsv\",\"w\")\n",
    "for iteration in ['500','600','700','800','900']:\n",
    "    sent_model_info = msgpack.load(open(os.path.join(output_dir,\"sent_res_final\", iteration+\"_sent_basic.mpack\")))\n",
    "    eta = np.load(os.path.join(output_dir,\"assoc_res_final\",iteration+\"_assoc_eta.npy\"))\n",
    "    phi = np.load(os.path.join(output_dir,\"sent_res_final\", iteration+\"_sent_phi.npy\"))\n",
    "    sent_ids_to_index = sent_model_info['ids_to_index']\n",
    "    sent_index_to_ids = sent_model_info['index_to_ids']\n",
    "\n",
    "    log_eq = log(1./float(n_identities))\n",
    "    def run_user(data):\n",
    "        uid, test_vector,training_vector, test_raw_text, test_deflection_strings,\\\n",
    "            test_identities_per_tweet, u_phi,u_eta,simple_assoc,simple_sent,usimpsent_uid = data\n",
    "\n",
    "        perpl = Counter()\n",
    "        spot = Counter()\n",
    "        n_obs = 0\n",
    "        user_simple_sent = SimpleSent(\"u_sent\", simple_sent_vector, id_to_index, rescale_value=rsv, \n",
    "                                      user_sent_info=usimpsent_uid, power_val=pv)\n",
    "        sent_models = [simp_sent,user_simple_sent]\n",
    "\n",
    "        test_vector = test_vector\n",
    "        tr_vec = training_vector\n",
    "        user_assoc = SimpleMult(\"user_assoc\",(tr_vec + SMOOTHING_PARAM) / \n",
    "                                       (tr_vec.sum() + n_identities * SMOOTHING_PARAM),div_by_sum=False)\n",
    "\n",
    "        our_assoc = SimpleMult(\"our_assoc\",softmax(u_eta))\n",
    "\n",
    "        #perplexity for associative models\n",
    "        test_sum = test_vector.sum()\n",
    "        for m in [simp_assoc, user_assoc, our_assoc]:\n",
    "            perpl[m.name()] += (test_vector * m.log_prob()).sum()\n",
    "\n",
    "\n",
    "        # set values for user\n",
    "        user_values = {v : u_phi[s_iter] for s_iter,v in enumerate(sent_index_to_ids) }\n",
    "        our_sent = OurSent(ids_in_index_order, user_values)\n",
    "\n",
    "        for tw_iter, test_text in enumerate(test_raw_text):\n",
    "            # get deflection string\n",
    "            test_deflection_str = test_deflection_strings[tw_iter].replace(\"uv.\",\"self.uv.\")\n",
    "\n",
    "            for identity in test_identities_per_tweet[tw_iter]:\n",
    "                n_obs += 1\n",
    "                index_of_identity = id_to_index[identity]\n",
    "\n",
    "                # get sentiment with this identity word replaced w/ placeholder\n",
    "                tmp = extract_tokens_twokenize_and_regex(test_text.decode(\"utf8\"),[], [],\n",
    "                                       make_lowercase=False,\n",
    "                                       do_lemmatize=False,\n",
    "                                       remove_possessive=False,\n",
    "                                       do_arabic_stemming=False)\n",
    "                id_test_text = \" \".join([x \n",
    "                                         if x.lower() not in [id_to_identity[identity],id_to_identity[identity]+\"s\"]\n",
    "                                        else 'identity' for x in tmp ])\n",
    "                if ' ' in id_to_identity[identity]:\n",
    "                    id_test_text = id_test_text.replace(id_to_identity[identity],\"compound identity\")\n",
    "                    id_test_text = id_test_text.replace(id_to_identity[identity]+'s',\"compound identity\")\n",
    "                test_sent = sentiment(id_test_text )['compound']\n",
    "\n",
    "\n",
    "                for m in sent_models:\n",
    "                    name = m.name()\n",
    "                    probs = m.compute_prob(test_sent)\n",
    "                    perpl[name] += log(probs[index_of_identity])\n",
    "                    spot[name] += np.where((-probs).argsort() == index_of_identity)[0][0]\n",
    "\n",
    "                if (not len(test_deflection_str) or\n",
    "                            (identity+'e' not in test_deflection_str and\n",
    "                             identity+'p' not in test_deflection_str and\n",
    "                             identity+'a' not in test_deflection_str)):\n",
    "                    perpl['our_sent'] += log_eq\n",
    "                    spot['our_sent'] += n_identities/2.\n",
    "                else:\n",
    "                    se_prob = our_sent.compute_prob(identity, test_deflection_str)\n",
    "                    perpl['our_sent'] += log(se_prob[index_of_identity])\n",
    "                    spot['our_sent'] += np.where((-se_prob).argsort() == index_of_identity)[0][0]\n",
    "\n",
    "        return [[spot, perpl, n_obs]]\n",
    "\n",
    "    pv = 1\n",
    "    rsv = 1\n",
    "\n",
    "    n_identities = len(identities)\n",
    "    simp_sent = SimpleSent(\"sent_basic\", simple_sent_vector, id_to_index, rescale_value=rsv, power_val=pv)\n",
    "    simp_assoc = SimpleMult(\"simp_assoc\",training_user_counts.sum(axis=0))\n",
    "\n",
    "    n_tot_obs = 0\n",
    "    data = [[users[i].uid, \n",
    "             users[i].test_vector,\n",
    "             users[i].training_vector, \n",
    "             users[i].test_raw_text, \n",
    "             users[i].test_deflection_strings,\n",
    "             users[i].test_identities_per_tweet,\n",
    "             phi[i],eta[i],simp_assoc,simp_sent,\n",
    "             user_simple_sent_data[int(users[i].uid)]] for i in range(len(users))]\n",
    "\n",
    "    perpl = Counter()\n",
    "    spot_dat = Counter()\n",
    "    d = sc.parallelize(data,256).flatMap(run_user).collect()\n",
    "\n",
    "    for dat in d:\n",
    "        spot, ppl, nobs = dat\n",
    "        perpl.update(ppl)\n",
    "        spot_dat.update(spot)\n",
    "        n_tot_obs += nobs\n",
    "    \n",
    "    for k, v in perpl.items():\n",
    "        of.write( tsn([iteration, k, v/float(n_tot_obs), spot_dat[k]/float(n_tot_obs)]))\n",
    "        print tsn([iteration, k, v/float(n_tot_obs), spot_dat[k]/float(n_tot_obs)])\n",
    "\n",
    "    print iteration, 'done'\n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}