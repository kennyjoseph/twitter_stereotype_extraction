{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import io\n",
    "import sys\n",
    "import codecs\n",
    "from user import *\n",
    "import glob\n",
    "import cPickle as pickle\n",
    "import os\n",
    "from twitter_dm.utility.general_utils import read_grouped_by_newline_file\n",
    "from twitter_dm.utility.general_utils import tab_stringify_newline as tsn\n",
    "from collections import defaultdict, Counter\n",
    "from textunit import TextUnit\n",
    "from constraints import get_id_and_value_map\n",
    "from constraints import IDENTITY_PREFIX, SENTWORD_PREFIX\n",
    "from math import log\n",
    "from vaderSentiment.vaderSentiment import sentiment\n",
    "from pred_models import *\n",
    "from stat_util import *\n",
    "from math import log\n",
    "from vaderSentiment.vaderSentiment import sentiment\n",
    "from twitter_dm.nlp.Tokenize import extract_tokens_twokenize_and_regex\n",
    "\n",
    "SMOOTHING_PARAM = 1\n",
    "sys.stdout = codecs.getwriter('utf-8')(sys.stdout)\n",
    "float_formatter = lambda x: \"%.6f\" % x\n",
    "np.set_printoptions(threshold=10000,\n",
    "                    linewidth=100,\n",
    "                    formatter={'float_kind':float_formatter})\n",
    "\n",
    "output_dir = \"../output/run_4/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identities = [x.strip() for x in io.open(\"../data/identity_data/final_identities_list.txt\").readlines()]\n",
    "identity_to_id = {identity : IDENTITY_PREFIX+str(i) for i, identity in enumerate(identities)}\n",
    "id_to_identity = {v : k for k, v in identity_to_id.items()}\n",
    "\n",
    "index_to_id = {int(x.split(\"\\t\")[0]):x.strip().split(\"\\t\")[1] \n",
    "               for x in  io.open(os.path.join(output_dir,\"index_to_id_final.tsv\"))}\n",
    "id_to_index = {v : k for k,v in index_to_id.items()}\n",
    "\n",
    "ids_in_index_order = [None] * len(identities)\n",
    "for k, v in index_to_id.items():\n",
    "    ids_in_index_order[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading user data...\n",
      " ... \n",
      " loaded! \n"
     ]
    }
   ],
   "source": [
    "users = list(load_users(output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44886"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dout = io.open(os.path.join(output_dir, \"user_uids.txt\"),\"w\")\n",
    "for u in users:\n",
    "    dout.write(unicode(u.uid) + \"\\n\")\n",
    "dout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# construct simple sentiment info for simple sent models\n",
    "i2ind = pd.DataFrame(id_to_index.items())\n",
    "i2ind.columns = ['iden_id','index']\n",
    "simple_sent_data = pd.read_csv(os.path.join(output_dir,\"user_to_identity_to_simple_sent_val.csv\"))\n",
    "\n",
    "overall_simple_sent = simple_sent_data[['iden_id','val']].groupby(\"iden_id\").mean().reset_index()\n",
    "overall_simple_sent = pd.merge(overall_simple_sent,i2ind, on='iden_id')\n",
    "simple_sent_vector = np.zeros(len(i2ind))\n",
    "for x in overall_simple_sent.values.tolist():\n",
    "    simple_sent_vector[x[2]] = x[1]\n",
    "\n",
    "user_simple_sent_data = defaultdict(dict)\n",
    "for x in simple_sent_data.values.tolist():\n",
    "    if x[2] in id_to_index:\n",
    "        user_simple_sent_data[x[1]][x[2]] = x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in some other data we're going to need\n",
    "training_user_counts = np.zeros((len(users),len(identity_to_id)))\n",
    "test_user_counts = np.zeros((len(users), len(identity_to_id)))\n",
    "for i, u in enumerate(users):\n",
    "    training_user_counts[i] = u.training_vector\n",
    "    test_user_counts[i] = u.test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load up spark\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (SparkConf().setMaster(\"local[*]\").setAppName(\"My app\")\n",
    "        .set(\"spark.local.dir\", \"/usr1/kjoseph/spark_tmp\"))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_user(data):\n",
    "    uid, test_vector,training_vector, test_raw_text, test_deflection_strings,\\\n",
    "        test_identities_per_tweet, u_phi,u_eta,simple_assoc,simple_sent,usimpsent_uid = data\n",
    "\n",
    "    perpl = Counter()\n",
    "    spot = Counter()\n",
    "    n_obs = 0\n",
    "    user_simple_sent = SimpleSent(\"u_sent\", simple_sent_vector, id_to_index, rescale_value=rsv, \n",
    "                                  user_sent_info=usimpsent_uid, power_val=pv)\n",
    "    sent_models = [simp_sent,user_simple_sent]\n",
    "    user_assoc = SimpleMult(\"user_assoc\",(training_vector + SMOOTHING_PARAM) / \n",
    "                                   (training_vector.sum() + n_identities * SMOOTHING_PARAM),div_by_sum=False)\n",
    "\n",
    "    our_assoc = SimpleMult(\"our_assoc\",softmax(u_eta))\n",
    "\n",
    "    #perplexity for associative models\n",
    "    for m in [simp_assoc, user_assoc, our_assoc]:\n",
    "        perpl[m.name()] += (test_vector * m.log_prob()).sum()\n",
    "\n",
    "    # set values for user\n",
    "    user_values = {v : u_phi[s_iter] for s_iter,v in enumerate(sent_index_to_ids) }\n",
    "    our_sent = OurSent(ids_in_index_order, user_values)\n",
    "\n",
    "    for tw_iter, test_text in enumerate(test_raw_text):\n",
    "        # get deflection string\n",
    "        test_deflection_str = test_deflection_strings[tw_iter].replace(\"uv.\",\"self.uv.\")\n",
    "\n",
    "        for identity in test_identities_per_tweet[tw_iter]:\n",
    "            n_obs += 1\n",
    "            index_of_identity = id_to_index[identity]\n",
    "\n",
    "            # get sentiment with this identity word replaced w/ placeholder\n",
    "            tmp = extract_tokens_twokenize_and_regex(test_text.decode(\"utf8\"),[], [],\n",
    "                                   make_lowercase=False,\n",
    "                                   do_lemmatize=False,\n",
    "                                   remove_possessive=False,\n",
    "                                   do_arabic_stemming=False)\n",
    "            id_test_text = \" \".join([x \n",
    "                                     if x.lower() not in [id_to_identity[identity],id_to_identity[identity]+\"s\"]\n",
    "                                    else 'identity' for x in tmp ])\n",
    "            if ' ' in id_to_identity[identity]:\n",
    "                id_test_text = id_test_text.replace(id_to_identity[identity],\"compound identity\")\n",
    "                id_test_text = id_test_text.replace(id_to_identity[identity]+'s',\"compound identity\")\n",
    "            test_sent = sentiment(id_test_text )['compound']\n",
    "\n",
    "\n",
    "            for m in sent_models:\n",
    "                name = m.name()\n",
    "                probs = m.compute_prob(test_sent)\n",
    "                try:\n",
    "                    perpl[name] += log(probs[index_of_identity])\n",
    "                except:\n",
    "                    pass\n",
    "                spot[name] += np.where((-probs).argsort() == index_of_identity)[0][0]\n",
    "\n",
    "            if (not len(test_deflection_str) or\n",
    "                        (identity+'e' not in test_deflection_str and\n",
    "                         identity+'p' not in test_deflection_str and\n",
    "                         identity+'a' not in test_deflection_str)):\n",
    "                perpl['our_sent'] += log_eq\n",
    "                spot['our_sent'] += n_identities/2.\n",
    "            else:\n",
    "                se_prob = our_sent.compute_prob(identity, test_deflection_str)\n",
    "                try:\n",
    "                    perpl['our_sent'] += log(se_prob[index_of_identity])\n",
    "                except:\n",
    "                    pass\n",
    "                spot['our_sent'] += np.where((-se_prob).argsort() == index_of_identity)[0][0]\n",
    "\n",
    "    return [[spot, perpl, n_obs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\tour_assoc\t-4.36447039092\t0.0\n",
      "\n",
      "300\tsimp_assoc\t-4.86241343742\t0.0\n",
      "\n",
      "300\tuser_assoc\t-4.47382903111\t0.0\n",
      "\n",
      "300\tsent_basic\t-5.71403519446\t134.885093981\n",
      "\n",
      "300\tu_sent\t-5.68876381475\t127.770838345\n",
      "\n",
      "300\tour_sent\t-10.4174328436\t126.30917173\n",
      "\n"
     ]
    }
   ],
   "source": [
    "of = open(os.path.join(output_dir,\"results.tsv\"),\"w\")\n",
    "sent_model_info = msgpack.load(open(os.path.join(output_dir,\"sent_res_final\", \"0_sent_basic.mpack\")))\n",
    "sent_ids_to_index = sent_model_info['ids_to_index']\n",
    "sent_index_to_ids = sent_model_info['index_to_ids']\n",
    "pv = 1\n",
    "rsv = 1\n",
    "log_eq = log(1./float(len(id_to_index)))\n",
    "n_identities = len(id_to_index)\n",
    "\n",
    "dout = io.open(os.path.join(output_dir, \"sent_ids_list.txt\"),\"w\")\n",
    "for s in sent_index_to_ids:\n",
    "    dout.write(unicode(s) + \"\\n\")\n",
    "dout.close()\n",
    "\n",
    "for iteration in ['300']:#,'600','700','800','900'\n",
    "    eta = np.load(os.path.join(output_dir,\"assoc_res_final\",iteration+\"_assoc_eta.npy\"))\n",
    "    #eta = np.load(os.path.join(output_dir,\"sent_res_final\", iteration+\"_sent_phi.npy\"))\n",
    "    phi = np.load(os.path.join(output_dir,\"sent_res_final\", iteration+\"_sent_phi.npy\"))\n",
    "\n",
    "    simp_sent = SimpleSent(\"sent_basic\", simple_sent_vector, id_to_index, rescale_value=rsv, power_val=pv)\n",
    "    simp_assoc = SimpleMult(\"simp_assoc\",training_user_counts.sum(axis=0))\n",
    "\n",
    "    data = [[users[i].uid, \n",
    "             users[i].test_vector,\n",
    "             users[i].training_vector, \n",
    "             users[i].test_raw_text, \n",
    "             users[i].test_deflection_strings,\n",
    "             users[i].test_identities_per_tweet,\n",
    "             phi[i],eta[i],simp_assoc,simp_sent,\n",
    "             user_simple_sent_data[int(users[i].uid)]] for i in range(len(users))]\n",
    "\n",
    "    \n",
    "    d = sc.parallelize(data,512).flatMap(run_user).collect()\n",
    "\n",
    "    perpl = Counter()\n",
    "    spot_dat = Counter()\n",
    "    n_tot_obs = 0\n",
    "    for dat in d:\n",
    "        spot, ppl, nobs = dat\n",
    "        perpl.update(ppl)\n",
    "        spot_dat.update(spot)\n",
    "        n_tot_obs += nobs\n",
    "    \n",
    "    for k, v in perpl.items():\n",
    "        of.write( tsn([iteration, k, v/float(n_tot_obs), spot_dat[k]/float(n_tot_obs)]))\n",
    "        print tsn([iteration, k, v/float(n_tot_obs), spot_dat[k]/float(n_tot_obs)])\n",
    "        \n",
    "of.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
